# A Deep Learning Approach to Author Obfuscation
*All resources, papers, besides useful links and some codes.*

Here the [slides](https://github.com/ozgurozdemir/deep-learning-approach-author-obfuscation/blob/master/A_Deep_Learning_Approach_To_Author_Obfuscation.pdf).

## Papers

**Author Obfuscation:**
* Machine translation approach - [Keswani et al., *Author Masking through Translation*, 2016](http://ceur-ws.org/Vol-1609/16090890.pdf)
* Stylometric feature extracting approach - [Mihaylova et al., *The Case for Being Average: A Mediocrity Approach to Style Masking and Author Obfuscation*, 2017](https://arxiv.org/abs/1707.03736v2)
* Deciding which stylometric feauture is important - [Patrick Juola and Darren Vescovi, *Analyzing Stylometric Approaches to Author Obfuscation*, 2011](https://link.springer.com/chapter/10.1007/978-3-642-24212-0_9)
* Overview of the Author Obfuscation - [Stein et al., *Author Obfuscation: Attacking the State of the Art in Authorship Verification*. n.d.](https://webis.de/downloads/publications/papers/stein_2016k.pdf)

**Recurrent Neural Network:**
* General information about RNN structure and usage - [Sutskever et al., *An Empirical Exploration of Recurrent Network Architectures*, 2015](http://proceedings.mlr.press/v37/jozefowicz15.pdf)

**Sequence to Sequence:**
* Initial paper of Seq2Seq Model - [Sutskever et al., *Sequence to Sequence Learning with Neural Networks*, 2014](https://arxiv.org/abs/1409.3215)
* Seq2Seq model for Machine Translation - [Cho et al., *Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation*, 2014](https://arxiv.org/abs/1406.1078)

**Attention Model:**
* Initial paper of Attention Model - [Bahdanau et al., *Neutral Machine Translation by Jointly Learning to Align and Translate*, 2016](https://arxiv.org/abs/1409.0473)
* Attention mechanism for Image Captioning - [Xu et al., *Show, Attend and Tell: Neural Image 	Caption Generation with Visual Attention*, 2016](https://arxiv.org/abs/1502.03044)
* Attention mechanism for Machine Translation - [Luong et al., *Effective Approaches to Attention-based Neural Machine Translation*, 2015](https://nlp.stanford.edu/pubs/emnlp15_attn.pdf)

**Paraphrase Generation:**
* Generator and Evaluator model - [Li et al., *Paraphrase Generation with Deep Reinforcement Learning*, 2018](https://arxiv.org/abs/1711.00279)

## Books

* Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep learning*. Cambridge, Massachusetts; London, England: The MIT Press, 2016 - [Bilgi Library](http://0-search.ebscohost.com.opac.bilgi.edu.tr/login.aspx?direct=true&db=cat03166a&AN=bil.b2450832&site=eds-live), [Amazon](https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618/ref=sr_1_3?ie=UTF8&qid=1542581950&sr=8-3&keywords=deep+learning), [Website](https://www.deeplearningbook.org/)
* Raschka, S., & Mirajalili, V. (2017). *Python machine learning : machine learning and deep learning with Python, scikit-learn, and TensorFlow*. Birmingham, UK: Packt Publishing, 2017 - [Bilgi Library](http://eds.b.ebscohost.com/eds/detail/detail?vid=6&sid=2d315835-8545-43d2-9d25-27e36d9d4fdc%40pdc-v-sessmgr02&bdata=JnNpdGU9ZWRzLWxpdmU%3d#AN=bil.b2450834&db=cat03166a), [Amazon](https://www.amazon.com/Python-Machine-Learning-scikit-learn-TensorFlow/dp/1787125939/ref=sr_1_fkmr0_2?ie=UTF8&qid=1542582359&sr=8-2-fkmr0&keywords=Python+machine+learning+%3A+machine+learning+and+deep+learning+with+Python%2C+scikit-learn%2C+and+TensorFlow+%2F+Sebastian+Raschka%2C+Vahid+Mirajalili)

## Websites and Blogs

* TensorFlow [tutorials](https://www.tensorflow.org/tutorials/).
* Useful Seq2Seq codes from [Matvey Ezhov](https://github.com/ematvey) - https://github.com/ematvey/tensorflow-seq2seq-tutorials
* Very, very useful slides and lecture notes of [Stanford University course, CS20](https://github.com/chiphuyen/stanford-tensorflow-tutorials) - [Syllabus](https://web.stanford.edu/class/cs20si/syllabus.html)
* Andrej Karpathy [Blog](https://medium.com/@karpathy)
* Adit Deshpande [Blog](https://adeshpande3.github.io/adeshpande3.github.io/)

## Author 

*Özgür Özdemir* - [LinkedIn](https://www.linkedin.com/in/%C3%B6zg%C3%BCr-%C3%B6zdemir-668110144/)
